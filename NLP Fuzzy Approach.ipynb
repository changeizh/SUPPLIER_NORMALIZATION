{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import cluster\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# called in data_cleaning\n",
    "def cleansing_special_characters(txt):\n",
    "    seps = [' ',';',':','.',',','*','#','@','|','\\\\','-','_','?','%','!','^','(',')','$','=','+','\"','<','>',\"'\",]\n",
    "    default_sep = seps[0]\n",
    "    \n",
    "    for sep in seps[1:]:\n",
    "        txt = txt.replace(sep, default_sep)\n",
    "    re.sub(' +', ' ', txt)\n",
    "    temp_list = [i.strip() for i in txt.split(default_sep)]\n",
    "    temp_list = [i for i in temp_list if i]\n",
    "    return \" \".join(temp_list)\n",
    "\n",
    "# =================================================================================================\n",
    "\n",
    "# called in data_cleaning\n",
    "def clean_stopword(txt):\n",
    "    temp_list = txt.split(\" \")\n",
    "    temp_list = [i for i in temp_list if i not in stopwords.words('english')]\n",
    "    return \" \".join(temp_list)\n",
    "\n",
    "# =================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# called in company_clusters\n",
    "def data_cleaning(data,nameCol , dropForeign =True):\n",
    "    data.dropna(subset=[nameCol], inplace =True)\n",
    "    data = data.rename_axis(\"CompanyID\").reset_index()\n",
    "    data['nonAscii_count'] = data[nameCol].apply(lambda x : sum([not c.isascii() for c in x]))\n",
    "    if dropForeign:\n",
    "        data = data[data.nonAscii_count ==0]\n",
    "    else:\n",
    "        pass\n",
    "    data.drop('nonAscii_count', axis =1, inplace= True)\n",
    "    data_clean = data.copy()\n",
    "    data_clean['CustomerName_clean'] = data_clean[nameCol].apply(lambda x: x.lower())\n",
    "    data_clean['CustomerName_clean'] = data_clean['CustomerName_clean'].apply(cleansing_special_characters)\n",
    "    data_clean['CustomerName_clean'] = data_clean['CustomerName_clean'].apply(clean_stopword)\n",
    "    return data_clean\n",
    "\n",
    "# ===========================================================================================\n",
    "\n",
    "# called in company_clusters\n",
    "def fuzz_similarity(cust_names):\n",
    "    similarity_array = np.ones((len(cust_names),(len(cust_names))))*100\n",
    "    \n",
    "    for i in range(1,len(cust_names)):\n",
    "        for j in range(i):\n",
    "            s1 = fuzz.token_set_ratio(cust_names[i],cust_names[j]) + 0.000000000001\n",
    "            s2 = fuzz.partial_ratio(cust_names[i],cust_names[j]) + 0.00000000001\n",
    "            similarity_array[i][j] = 2*s1*s2/(s1+s2)\n",
    "#             print(similarity_array[i][j])\n",
    "    \n",
    "    for i in range(len(cust_names)):\n",
    "        for j in range(i+1, len(cust_names)):\n",
    "            similarity_array[i][j] = similarity_array[j][i]\n",
    "    \n",
    "    np.fill_diagonal(similarity_array,100)\n",
    "    return similarity_array\n",
    "\n",
    "# ========================================================================\n",
    "\n",
    "#output to standard_name\n",
    "def company_clusters(data, nameCol, dropForeign = True):\n",
    "    data_clean = data_cleaning(data, nameCol=nameCol, dropForeign=dropForeign)\n",
    "    cust_names = data_clean.CustomerName_clean.to_list()\n",
    "    cust_ids = data_clean.CompanyID.to_list()\n",
    "    \n",
    "    similarity_array = fuzz_similarity(cust_names)\n",
    "    clusters = cluster.AffinityPropagation(affinity = 'precomputed').fit_predict(similarity_array)\n",
    "    df_cluster = pd.DataFrame(list(zip(cust_ids, clusters)), columns=['CompanyID','cluster'])\n",
    "    \n",
    "    df_eval = df_cluster.merge(data_clean, on = 'CompanyID',how='left')\n",
    "    return df_eval\n",
    "\n",
    "\n",
    "def standard_name(df_eval):\n",
    "    d_standard_name = {}\n",
    "    for cluster in df_eval.cluster.unique():\n",
    "        names = df_eval[df_eval['cluster']==cluster].CustomerName_clean.to_list()\n",
    "        l_common_substring = []\n",
    "        if len(names)>1:\n",
    "            for i in range(0, len(names)):\n",
    "                for j in range(i+1, len(names)):\n",
    "                    seqMatch = SequenceMatcher(None, names[i],names[j])\n",
    "                    match = seqMatch.find_longest_match(0, len(names[i]), 0, len(names[j]))\n",
    "                    if (match.size!= 0):\n",
    "                        l_common_substring.append(names[i][match.a :match.a + match.size].strip())\n",
    "            n = len(l_common_substring)\n",
    "            counts = Counter(l_common_substring)\n",
    "            get_mode = dict(counts)\n",
    "            mode = [k for k,v in get_mode.items() if v == max(list(counts.values()))]\n",
    "            d_standard_name[cluster] = \";\".join(mode)\n",
    "        else:\n",
    "            d_standard_name[cluster] = names[0]\n",
    "            \n",
    "    df_standard_names = pd.DataFrame(list(d_standard_name.items()), columns=['cluster', 'StandardName'])\n",
    "    df_eval = df_eval.merge(df_standard_names, on = 'cluster', how = 'left')\n",
    "    df_eval['Score_with_standard'] = df_eval.apply(lambda x: fuzz.token_set_ratio(x['StandardName'],x['CustomerName_clean']),axis =1)\n",
    "    df_eval['standard_name_withoutSpaces'] = df_eval.StandardName.apply(lambda x: x.replace(\" \",\"\"))\n",
    "    \n",
    "    for name in df_eval.standard_name_withoutSpaces.unique():\n",
    "        if len(df_eval[df_eval.standard_name_withoutSpaces==name].cluster.unique()) > 1:\n",
    "            df_eval.loc[df_eval.standard_name_withoutSpaces==name, 'StandardName'] = name\n",
    "            \n",
    "    return df_eval.drop('standard_name_withoutSpaces', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "supp = pd.read_excel('2000_Aug_Imp-20.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manish\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_affinity_propagation.py:226: ConvergenceWarning: Affinity propagation did not converge, this model will not have any cluster centers.\n",
      "  \"will not have any cluster centers.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "column_name ='BUYER_NAME'\n",
    "final = standard_name(company_clusters(supp,column_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_excel('test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
